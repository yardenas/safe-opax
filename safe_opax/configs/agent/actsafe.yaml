defaults:
  - penalizer: lbsgd

name: actsafe
replay_buffer:
  batch_size: 16
  sequence_length: 50
  capacity: 1000
sentiment:
  ensemble_size: 5
  model_initialization_scale: null
  constraint_pessimism: null
  objective_optimism: null
model:
  hidden_size: 200
  stochastic_size: 60
  deterministic_size: 200
actor:
  n_layers: 4
  hidden_size: 400
  init_stddev: 5.
  initialization_scale: 0.01
critic:
  n_layers: 3
  hidden_size: 400
actor_optimizer:
  lr: 8e-5
  eps: 1e-5
  clip: 100.0
critic_optimizer:
  lr: 8e-5
  eps: 1e-5
  clip: 100.0
safety_critic_optimizer:
  lr: 8e-5
  eps: 1e-5
  clip: 100.0
model_optimizer:
  lr: 3e-4
  eps: 1e-5
  clip: 100.0
discount: 0.99
safety_discount: 0.99
lambda_: 0.95
plan_horizon: 15
update_steps: 100
train_every: 1000
beta: 1.0
free_nats: 1.
kl_mix: 0.8
safety_slack: 0.
evaluate_model: false
exploration_strategy: uniform
exploration_steps: 5000
offline_steps: 200000
learn_model_steps: null
exploration_reward_scale: 10.0
exploration_epistemic_scale: 1.
unsupervised: false
reward_scale: 1.